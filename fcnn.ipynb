{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOh0OvkF-u3y",
        "outputId": "056bf554-fa2a-44fb-f373-fb66d37d6d5e"
      },
      "source": [
        "!pip install idx2numpy\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: idx2numpy in /usr/local/lib/python3.7/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from idx2numpy) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from idx2numpy) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOL90EuE8OEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed96cef0-d476-4eda-89d4-4903344881d9"
      },
      "source": [
        "import numpy as np\r\n",
        "import idx2numpy\r\n",
        "\r\n",
        "train_data=idx2numpy.convert_from_file('/content/train-images-idx3-ubyte')\r\n",
        "train_data = np.reshape(train_data,(60000,28*28))\r\n",
        "train_label = idx2numpy.convert_from_file('/content/train-labels-idx1-ubyte')\r\n",
        "test_data=idx2numpy.convert_from_file('/content/t10k-images-idx3-ubyte')\r\n",
        "test_data = np.reshape(test_data,(10000, 28*28))\r\n",
        "test_label = idx2numpy.convert_from_file('/content/t10k-labels-idx1-ubyte')\r\n",
        "print(train_data.shape)\r\n",
        "print(train_label.shape)\r\n",
        "print(test_data.shape)\r\n",
        "print(test_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOfnLSXiDJzJ"
      },
      "source": [
        "def ReLU(x):\r\n",
        "  return max(0,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HATO9ooUb8Z"
      },
      "source": [
        "def ReLUAll(a):\r\n",
        "  temp = a.copy()\r\n",
        "  for i in range(a.shape[0]):\r\n",
        "    temp[i] = ReLU(temp[i])\r\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJeqvcBf97TC"
      },
      "source": [
        "def softmax(v):\r\n",
        "  newv = v.copy()\r\n",
        "  newv = np.exp(newv)\r\n",
        "  tot = sum(newv)\r\n",
        "  newv /= tot\r\n",
        "  return newv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC3FjKPuElfv"
      },
      "source": [
        "def genWB(size, n = 1):\r\n",
        "  \"\"\"\r\n",
        "  generates the weight and bias for n hidden layers and the final weight. Default of direct input-output\r\n",
        "  returns w, b (weight, bias)\r\n",
        "  \"\"\"\r\n",
        "  w = []\r\n",
        "  b = []\r\n",
        "  # hidden layers\r\n",
        "  for i in range(n):\r\n",
        "    tempw = np.random.randn(size,size) * np.sqrt(1.0 / size)\r\n",
        "    tempb = np.zeros((size, 1)) \r\n",
        "    w.append(tempw)\r\n",
        "    b.append(tempb)\r\n",
        "  \r\n",
        "\r\n",
        "  # final layer\r\n",
        "  tempw = np.random.randn(10,size) * np.sqrt(1.0 / size)\r\n",
        "  tempb = np.zeros((10, 1)) \r\n",
        "  w.append(tempw)\r\n",
        "  b.append(tempb)\r\n",
        "\r\n",
        "  w = np.array(w)\r\n",
        "  b = np.array(b)\r\n",
        "  return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoiVTZBWxRdU"
      },
      "source": [
        "def CEE(y, target):\r\n",
        "  loss = -np.log(y)\r\n",
        "  return loss[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWfyDU2_FRPY"
      },
      "source": [
        "def CELF(y, target):\r\n",
        "  return sum(CEE(y, target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mx2FgbMTOUf"
      },
      "source": [
        "def forward(x, w, b):\r\n",
        "  temp = x.copy()\r\n",
        "  temp = temp.reshape(x.shape[0],1)\r\n",
        "  a = w @ temp + b\r\n",
        "  return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHRRZWXFuffI"
      },
      "source": [
        "def activation(a, t):\r\n",
        "  \"\"\"\r\n",
        "  activation function. a is input, t is decider for relu(1)/softmax(0)\r\n",
        "  \"\"\"\r\n",
        "  if t:\r\n",
        "    return ReLUAll(a)\r\n",
        "  else:\r\n",
        "    return softmax(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esGhM31xyMlt"
      },
      "source": [
        "def forwardpass(x, w, b):\r\n",
        "  z = []\r\n",
        "  z.append(x)\r\n",
        "  for i in range(w.shape[0]-1):\r\n",
        "    c = forward(x, w[i], b[i])\r\n",
        "    c = activation(c, 1)\r\n",
        "    z.append(c)\r\n",
        "  end = forward(c, w[-1], b[-1])\r\n",
        "  end = activation(end, 0)\r\n",
        "  z.append(end)\r\n",
        "  z = np.array(z)\r\n",
        "  return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PGv8lJZ5E7f"
      },
      "source": [
        "def genError(z, w, b, target):\r\n",
        "  error = []\r\n",
        "  eL = z[-1].copy()\r\n",
        "  eL[target] -= 1\r\n",
        "  error.append(eL)\r\n",
        "  ei = eL.copy()\r\n",
        "  for i in range((w.shape[0]-1),0,-1):\r\n",
        "    ei = np.multiply(w[i].T @ ei, z[i])\r\n",
        "    error.append(ei)\r\n",
        "  error = np.array(error)\r\n",
        "  return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXZEdoXRGg2K"
      },
      "source": [
        "def gradient(error, z):\r\n",
        "  gw = []\r\n",
        "  gb = []\r\n",
        "  errorflip = error.copy()\r\n",
        "  errorflip = np.flip(errorflip)\r\n",
        "  for i in range(error.shape[0]):\r\n",
        "    grad = errorflip[i] @ z[i].T\r\n",
        "    gw.append(grad)\r\n",
        "    gb.append(errorflip[i])\r\n",
        "  gw = np.array(gw)\r\n",
        "  gb = np.array(gb)\r\n",
        "  return gw, gb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svJtpsuJrKIu"
      },
      "source": [
        "def gen_mBatches(X, Y, batch_size):\r\n",
        "    mini_batches = []\r\n",
        "    num_batch = X.shape[0] // batch_size\r\n",
        "    data = np.hstack((X, Y)) \r\n",
        "    np.random.shuffle(data)\r\n",
        "    for i in range(num_batch+1):\r\n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :] \r\n",
        "        X_mini = mini_batch[:, :-1] \r\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \r\n",
        "        mini_batches.append((X_mini, Y_mini)) \r\n",
        "    return mini_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqSIICJbbQIC"
      },
      "source": [
        "def cycle(x, w, b, target):\r\n",
        "  temp = x.copy()\r\n",
        "  temp = temp.reshape(x.shape[0],1)\r\n",
        "  z = forwardpass(temp, w, b)\r\n",
        "  e = genError(z, w, b, target)\r\n",
        "  gw, gb = gradient(e, z)\r\n",
        "  return gw, gb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4mM4vVwNh-A"
      },
      "source": [
        "def minisgd(x,y ,w, b, epochnum, batchsize, learn_rate):\r\n",
        "  sumgw = w - w\r\n",
        "  sumgb = b - b\r\n",
        "  for i in range(epochnum):\r\n",
        "    minib = gen_mBatches(x, y, batchsize)\r\n",
        "    for item in minib:\r\n",
        "      miniX, miniY = item\r\n",
        "      if miniX.shape[0] == 0:\r\n",
        "        continue\r\n",
        "      for j in range(miniX.shape[0]):\r\n",
        "        gw, gb = cycle(miniX[j], w, b, miniY[j])\r\n",
        "        sumgw = sumgw + gw\r\n",
        "        sumgb = sumgb + gb\r\n",
        "      w = w - ((learn_rate/miniX.shape[0]) * sumgw)\r\n",
        "      b = b - ((learn_rate/miniX.shape[0]) * sumgb)\r\n",
        "      sumgw = w - w\r\n",
        "      sumgb = b - b\r\n",
        "  return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qg9jyEflj8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa3f599-bb17-4d1f-a48f-43a76135d867"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "  w, b = genWB(train_data[0].shape[0],1)\r\n",
        "  lr = 0.0000001\r\n",
        "  train_copy = train_label.copy()\r\n",
        "  train_copy = train_copy.reshape(train_copy.shape[0], 1)\r\n",
        "  fw, fb = minisgd(train_data, train_copy, w, b, 2, 10, lr)\r\n",
        "  ppp = test_data[:1000]\r\n",
        "  qqq = test_label[:1000]\r\n",
        "  correct = 0\r\n",
        "  tot = ppp.shape[0]\r\n",
        "  for itm in range(tot):\r\n",
        "    a = ppp[itm].copy()\r\n",
        "    a = a.reshape(a.shape[0],1)\r\n",
        "    p = forwardpass(a, fw, fb)\r\n",
        "    choice = np.argmax(p[-1])\r\n",
        "    if choice == qqq[itm]:\r\n",
        "      correct += 1\r\n",
        "  print(\"==hit rate==\")\r\n",
        "  print(correct/tot)\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==hit rate==\n",
            "0.865\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}